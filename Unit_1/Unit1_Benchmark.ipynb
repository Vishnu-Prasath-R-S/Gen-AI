{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "smRnQRB-1-vY"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNu5UnLT2cAE",
        "outputId": "1b1a3766-a444-4126-c88d-5c8c6ee5770e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "BERT output:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'generated_text': 'The future of Artificial Intelligence is....................'}]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "RoBERTa output:\n",
            "[{'generated_text': 'The future of Artificial Intelligence is'}]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "BART output:\n",
            "[{'generated_text': 'The future of Artificial Intelligence isbeginbeginbegin jur Patricia Patricia 2014 Reaction Information Informationntax edited po Nazis edited Nazis Writer Writer�'}]\n"
          ]
        }
      ],
      "source": [
        "#PES2UG23CS703 EXPERIMENT 1 PROMPTING\n",
        "prompt = \"The future of Artificial Intelligence is\"\n",
        "\n",
        "models = {\n",
        "    \"BERT\": \"bert-base-uncased\",\n",
        "    \"RoBERTa\": \"roberta-base\",\n",
        "    \"BART\": \"facebook/bart-base\"\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    try:\n",
        "        gen = pipeline(\"text-generation\", model=model)\n",
        "        print(f\"\\n{name} output:\")\n",
        "        print(gen(prompt, max_new_tokens=20))\n",
        "    except Exception as e:\n",
        "        print(f\"\\n{name} failed:\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWYau05x2tV-",
        "outputId": "488678ef-a58e-4a0a-e46f-69d756e847b0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "BERT predictions:\n",
            "[{'score': 0.5396932363510132, 'token': 3443, 'token_str': 'create', 'sequence': 'the goal of generative ai is to create new content.'}, {'score': 0.15575720369815826, 'token': 9699, 'token_str': 'generate', 'sequence': 'the goal of generative ai is to generate new content.'}, {'score': 0.05405500903725624, 'token': 3965, 'token_str': 'produce', 'sequence': 'the goal of generative ai is to produce new content.'}, {'score': 0.04451530799269676, 'token': 4503, 'token_str': 'develop', 'sequence': 'the goal of generative ai is to develop new content.'}, {'score': 0.01757744885981083, 'token': 5587, 'token_str': 'add', 'sequence': 'the goal of generative ai is to add new content.'}]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "RoBERTa predictions:\n",
            "\n",
            "RoBERTa failed: No mask_token (<mask>) found on the input\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "BART predictions:\n",
            "\n",
            "BART failed: No mask_token (<mask>) found on the input\n"
          ]
        }
      ],
      "source": [
        "#PES2UG23CS703 EXPERIMENT 2\n",
        "sentence = \"The goal of Generative AI is to [MASK] new content.\"\n",
        "\n",
        "for name, model in models.items():\n",
        "    try:\n",
        "        fm = pipeline(\"fill-mask\", model=model)\n",
        "        print(f\"\\n{name} predictions:\")\n",
        "        print(fm(sentence))\n",
        "    except Exception as e:\n",
        "        print(f\"\\n{name} failed:\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kxk84dGv28f6",
        "outputId": "0bcfbee1-4402-4aed-94e7-bc60cd2124e6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "BERT answer:\n",
            "{'score': 0.009666637051850557, 'start': 46, 'end': 82, 'answer': 'hallucinations, bias, and deepfakes.'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "RoBERTa answer:\n",
            "{'score': 0.004378531593829393, 'start': 14, 'end': 67, 'answer': 'poses significant risks such as hallucinations, bias,'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "BART answer:\n",
            "{'score': 0.03242884669452906, 'start': 14, 'end': 81, 'answer': 'poses significant risks such as hallucinations, bias, and deepfakes'}\n"
          ]
        }
      ],
      "source": [
        "#PES2UG23CS703 EXPERIMENT 3\n",
        "question = \"What are the risks?\"\n",
        "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        "\n",
        "for name, model in models.items():\n",
        "    try:\n",
        "        qa = pipeline(\"question-answering\", model=model)\n",
        "        print(f\"\\n{name} answer:\")\n",
        "        print(qa(question=question, context=context))\n",
        "    except Exception as e:\n",
        "        print(f\"\\n{name} failed:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VdW61KE5koQ"
      },
      "source": [
        "#PES2UG23CS703\n",
        "## Observation Table – Unit 1 Model Benchmark Results\n",
        "\n",
        "| Task        | Model    | Success / Failure | Observation | Architectural Reason |\n",
        "|------------|----------|-------------------|-------------|----------------------|\n",
        "| Generation | BERT     | Failure | Generated repeated symbols without meaningful continuation | Encoder-only architecture; not trained for autoregressive text generation |\n",
        "| Generation | RoBERTa  | Failure | Repeated the prompt without generating new tokens | Encoder-only model designed for understanding tasks |\n",
        "| Generation | BART     | Partial | Generated incoherent and noisy text | Encoder–decoder architecture, but not trained for causal language modeling |\n",
        "| Fill-Mask  | BERT     | Success | Correctly predicted words like “create” and “generate” | Trained using Masked Language Modeling with [MASK] token |\n",
        "| Fill-Mask  | RoBERTa  | Failure | Failed due to missing expected mask token | Uses a different tokenizer (<mask>), highlighting preprocessing dependency |\n",
        "| Fill-Mask  | BART     | Failure | Mask token not detected in input | Not primarily trained for MLM and uses different masking scheme |\n",
        "| QA         | BERT     | Partial | Extracted relevant answer span with low confidence | Not fine-tuned for question answering tasks |\n",
        "| QA         | RoBERTa  | Failure | Returned irrelevant or empty span | Base model not trained for extractive QA |\n",
        "| QA         | BART     | Partial | Extracted a keyword from context | Not fine-tuned for extractive question answering |\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
